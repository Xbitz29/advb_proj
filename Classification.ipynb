{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa98fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fecc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_add = pd.read_csv('train_add.csv')\n",
    "test_add = pd.read_csv('test_add.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f491b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataframe(df, date_column):\n",
    "    # Copy the original dataframe to avoid modifying the original\n",
    "    transformed_df = df.copy()\n",
    "\n",
    "    # Convert the date column to datetime type\n",
    "    transformed_df[date_column] = pd.to_datetime(transformed_df[date_column])\n",
    "\n",
    "    # Extract day, month, and year from the date column\n",
    "    transformed_df['day'] = transformed_df[date_column].dt.day\n",
    "    transformed_df['month'] = transformed_df[date_column].dt.month\n",
    "    transformed_df['year'] = transformed_df[date_column].dt.year\n",
    "\n",
    "    # Add day of the week column\n",
    "    # transformed_df['day_of_week'] = transformed_df[date_column].dt.weekday\n",
    "\n",
    "    # Remove the original date column\n",
    "    transformed_df.drop(date_column, axis=1, inplace=True)\n",
    "\n",
    "    return transformed_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = transform_dataframe(train_add, 'period_dt')\n",
    "test_data = transform_dataframe(test_add, 'period_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aea70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def transform_dt_df(df):\n",
    "    # Copy the original dataframe to avoid modifying the original\n",
    "    transformed_df = df.copy()\n",
    "\n",
    "    # Specify the columns to transform\n",
    "    columns_to_transform = [\n",
    "                            'VALID_FROM_DTTM_y']\n",
    "\n",
    "    # Transform the specified columns\n",
    "    for column in columns_to_transform:\n",
    "        transformed_df[column] = pd.to_datetime(transformed_df[column], format=\"%d%b%Y:%H:%M:%S\")\n",
    "\n",
    "        # Split date into day, month, and year\n",
    "        transformed_df[column + '_day'] = transformed_df[column].dt.day\n",
    "        transformed_df[column + '_month'] = transformed_df[column].dt.month\n",
    "        transformed_df[column + '_year'] = transformed_df[column].dt.year\n",
    "\n",
    "        # Split time into separate columns\n",
    "        transformed_df[column + '_hour'] = transformed_df[column].dt.hour\n",
    "        transformed_df[column + '_minute'] = transformed_df[column].dt.minute\n",
    "        transformed_df[column + '_second'] = transformed_df[column].dt.second\n",
    "\n",
    "        # Add weekday column\n",
    "        transformed_df[column + '_weekday'] = transformed_df[column].dt.weekday\n",
    "\n",
    "        # Ensure dates are no later than the year 2260\n",
    "        transformed_df.loc[transformed_df[column + '_year'] > 2260, [column + '_day', column + '_month', column + '_year']] = [1, 1, 2260]\n",
    "\n",
    "    # Remove the original string columns\n",
    "    transformed_df.drop(columns_to_transform, axis=1, inplace=True)\n",
    "\n",
    "    return transformed_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56543bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = transform_dt_df(train_data)\n",
    "test_data = transform_dt_df(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2729df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, D):\n",
    "    # Convert day, month, and year columns to datetime format\n",
    "    df['date'] = pd.to_datetime(df[['day', 'month', 'year']])\n",
    "    df.sort_values(by='date', inplace=True)\n",
    "    # Group the DataFrame by product_id and find the first occurrence\n",
    "    first_occurrence = df.groupby(by=['location_id', 'product_id'])['date'].first()\n",
    "    first_occurrence = pd.DataFrame(first_occurrence)\n",
    "    first_occurrence.reset_index(inplace=True)\n",
    "    first_occurrence.rename(columns={'date':'first_date'}, inplace=True)\n",
    "    # Merge the first occurrence back to the original DataFrame\n",
    "    df = df.merge(first_occurrence, on=['location_id','product_id'], suffixes=('', '_first'))\n",
    "\n",
    "    # Calculate the difference in days between subsequent occurrences and the first occurrence\n",
    "    df['date_diff'] = (df['date'] - df['first_date']).dt.days\n",
    "\n",
    "    # Filter out the products where the date difference is greater than D\n",
    "    filtered_df = df[df['date_diff'] <= D].copy()\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    filtered_df.drop(['first_date', 'date_diff'], axis=1, inplace=True)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "train_data = filter_df(train_data, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58de530",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавим процент от начальной цены\n",
    "train_data.loc[:, 'disc_percent'] = train_data['PRICE_AFTER_DISC'] / train_data['PRICE_REGULAR']\n",
    "test_data.loc[:, 'disc_percent'] = test_data['PRICE_AFTER_DISC'] / test_data['PRICE_REGULAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf341f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Средние значения ответа\n",
    "group_all = train_data[['month', 'location_id', 'demand']].groupby(by=['month', 'location_id'], as_index=False).mean()\n",
    "\n",
    "group_date = train_data[['month', 'demand']].groupby(by=['month'], as_index=False).mean()\n",
    "train_data = train_data.merge(group_all, on=['month', 'location_id'], how='left', suffixes=('', '_all'))\n",
    "test_data = test_data.merge(group_all, on=['month', 'location_id'], how='left', suffixes=('', '_all'))\n",
    "train_data = train_data.merge(group_date, on=['month'], how='left', suffixes=('', '_new'))\n",
    "test_data = test_data.merge(group_date, on=['month'], how='left', suffixes=('', '_new'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65573f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sort_values(by=['year', 'month', 'day'])\n",
    "tmp = train_data.groupby(by=['location_id', 'product_id'], as_index=False).cumcount() + 1\n",
    "train_data.loc[:, 'number_of_week'] = tmp\n",
    "test_data = test_data.sort_values(by=['year', 'month', 'day'])\n",
    "tmp = test_data.groupby(by=['location_id', 'product_id'], as_index=False).cumcount() + 1\n",
    "test_data.loc[:, 'number_of_week'] = tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e683ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(columns=['date'], inplace=True)\n",
    "train_data.drop(columns=['product_id'], inplace=True)\n",
    "test_data.drop(columns=['product_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = train_data.columns[(train_data.dtypes == 'object')].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c04f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def demand_bucketing(df_column, closest_points):\n",
    "    column = list(df_column)\n",
    "    ans = []\n",
    "    for elem in column:\n",
    "        if elem > 3:\n",
    "            ans.append(3)\n",
    "            continue\n",
    "        min_val = 100500\n",
    "        cor_point = 0\n",
    "        for it2, point in enumerate(closest_points):\n",
    "            if abs(point - elem) < min_val:\n",
    "                min_val = abs(point - elem)\n",
    "                cor_point = point\n",
    "        ans.append(cor_point)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823311da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[:, 'demand_class'] = demand_bucketing(train_data['demand'], [0, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d39cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "X, y = train_data.drop(columns=['demand', 'demand_class']), train_data['demand_class']\n",
    "data = Pool(X, y, cat_features=text_features)\n",
    "\n",
    "model_class1 = CatBoostClassifier(silent=True, random_seed=42, eval_metric='MCC', n_estimators=700, depth=6, learning_rate=0.15, l2_leaf_reg=0.5, cat_features=text_features)\n",
    "model_class1.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4390218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "X, y = train_data.drop(columns=['demand', 'demand_class']), train_data['demand_class']\n",
    "data = Pool(X, y, cat_features=text_features)\n",
    "\n",
    "model_class2 = CatBoostClassifier(silent=True, random_seed=42, eval_metric='MCC', n_estimators=700, depth=8, learning_rate=0.15, l2_leaf_reg=0.5, cat_features=text_features)\n",
    "model_class2.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793aea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model_class1.predict(test_data.drop(columns=['demand']))\n",
    "y_pred2 = model_class2.predict(test_data.drop(columns=['demand']))\n",
    "y_pred1 = np.where(y_pred1 <= 1, 1, 1.2)\n",
    "y_pred2 = np.where(y_pred2 <= 1, 1, 1.2)\n",
    "test_data.loc[:, 'demand'] = (y_pred1 + y_pred2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 14))\n",
    "sns.histplot(test_data['demand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458cffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['id', 'demand']].to_csv(\"ans.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
